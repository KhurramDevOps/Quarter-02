{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmY2s5zip2GA0tHDhdXsNB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhurramDevOps/Quarter-02/blob/master/Rag_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project: LangChain RAG with Google Gemini Flash and Pinecone**"
      ],
      "metadata": {
        "id": "3GMm_nIRZEKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-pinecone langchain-google-genai"
      ],
      "metadata": {
        "id": "4anGvSmstwJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc913e54-ad34-4f39-83bb-bd39d5097ade"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display\n",
        "from google.colab import userdata\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "pinecone_api_key = userdata.get('pinecone_api')\n",
        "pc = Pinecone(api_key=pinecone_api_key)"
      ],
      "metadata": {
        "id": "6WIBLSanuE0m"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Resetting and Recreating the Index  \n",
        "This snippet deletes the existing index named **\"practise-rag-project\"**, then recreates it with the same configuration. The new index is initialized for further operations.\n",
        "\n"
      ],
      "metadata": {
        "id": "3YgvHvW3aCdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"practice-rag-project\"\n",
        "pc.delete_index(index_name)\n",
        "\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=768,\n",
        "    metric='cosine',\n",
        "    spec  = ServerlessSpec(cloud=\"aws\",region=\"us-east-1\"),\n",
        ")\n",
        "\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "46Ggs_N1u4wo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Embeddings with Google Generative AI\n",
        "In this snippet, we use the `GoogleGenerativeAIEmbeddings` module from LangChain to generate vector embeddings for the query **\"Building a Rag project!\"**. The embeddings are computed using the **\"models/embedding-001\"** model from Google's Generative AI API. The first five values of the embedding vector are displayed as a preview.\n"
      ],
      "metadata": {
        "id": "M7lkCXA-aqPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import os\n",
        "\n",
        "GEMINI_KEY = userdata.get(\"GEMINI_API_KEY_2\")\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\",google_api_key=GEMINI_KEY )"
      ],
      "metadata": {
        "id": "3CyHNRdtwqTG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector = embeddings.embed_query(\"Building a Rag project! \")\n",
        "\n"
      ],
      "metadata": {
        "id": "hlltV7HQxP2S"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBlcW09p39TY",
        "outputId": "c91cf3b4-6062-4f69-86a4-da4f09f976ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.005886509083211422,\n",
              " -0.01920737698674202,\n",
              " -0.01310189813375473,\n",
              " -0.03790365159511566,\n",
              " -0.003551947884261608]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Vector Store with Pinecone and Documents  \n",
        "This snippet initializes a Pinecone vector store with Google AI embeddings and creates ten documents with diverse content and metadata. These documents are ready for semantic search or AI-powered analysis in the vector store.\n"
      ],
      "metadata": {
        "id": "D_PPU8xfbQVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store = PineconeVectorStore(index=index, embedding=embeddings)"
      ],
      "metadata": {
        "id": "_9sdtw5C4MgY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1 = Document(\n",
        "    page_content=\"I have chocolate chip pancake and scrambled eggs for breakfast\",\n",
        "    metadata={\"source\": \"personal\", \"meal\": \"breakfast\"}\n",
        ")\n",
        "\n",
        "document_2 = Document(\n",
        "    page_content=\"LangChain is a framework for building applications with LLMs (Large Language Models), such as GPT. It provides tools to manage chains, agents, and memory for building more advanced AI applications.\",\n",
        "    metadata={\"topic\": \"LangChain\", \"category\": \"AI Framework\"}\n",
        ")\n",
        "\n",
        "document_3 = Document(\n",
        "    page_content=\"Agentic AI refers to autonomous AI systems that are capable of decision-making, learning, and adaptation in real-world environments without needing constant human intervention.\",\n",
        "    metadata={\"topic\": \"Agentic AI\", \"category\": \"Artificial Intelligence\", \"importance\": \"High\"}\n",
        ")\n",
        "\n",
        "document_4 = Document(\n",
        "    page_content=\"In the latest world news, a new tech company has developed an innovative AI that is able to solve real-world problems faster than previous models, pushing the boundaries of automation and efficiency.\",\n",
        "    metadata={\"topic\": \"Tech News\", \"date\": \"2025-01-02\", \"company\": \"Innovative AI Company\"}\n",
        ")\n",
        "\n",
        "document_5 = Document(\n",
        "    page_content=\"The use of AI in healthcare is growing rapidly. Recent advancements in diagnostic AI tools are helping doctors identify diseases more accurately and faster, significantly improving patient outcomes.\",\n",
        "    metadata={\"topic\": \"Healthcare AI\", \"category\": \"Medical Technology\", \"impact\": \"Positive\"}\n",
        ")\n",
        "\n",
        "document_6 = Document(\n",
        "    page_content=\"LangChain offers a wide range of tools to work with LLMs. This includes support for document search, question-answering systems, and memory management to make intelligent agents more effective in real-world tasks.\",\n",
        "    metadata={\"topic\": \"LangChain\", \"category\": \"AI Tools\", \"use_case\": \"Advanced workflows\"}\n",
        ")\n",
        "\n",
        "document_7 = Document(\n",
        "    page_content=\"Agentic AI is becoming increasingly important in industries like autonomous vehicles, robotics, and smart cities, where real-time decision-making and adaptability are crucial for success.\",\n",
        "    metadata={\"topic\": \"Agentic AI\", \"category\": \"Industry Applications\", \"industries\": [\"Autonomous Vehicles\", \"Robotics\", \"Smart Cities\"]}\n",
        ")\n",
        "\n",
        "document_8 = Document(\n",
        "    page_content=\"A new world record has been set for the fastest internet speed, with researchers breaking through previous limitations using advanced fiber-optic technology, promising faster and more efficient global communication.\",\n",
        "    metadata={\"topic\": \"Tech News\", \"category\": \"Innovation\", \"record\": \"Fastest Internet Speed\", \"date\": \"2025-01-02\"}\n",
        ")\n",
        "\n",
        "document_9 = Document(\n",
        "    page_content=\"In a recent study, AI-powered chatbots have been shown to outperform human customer service agents in resolving technical issues, reducing wait times and improving customer satisfaction.\",\n",
        "    metadata={\"topic\": \"AI in Customer Service\", \"category\": \"Business Technology\", \"impact\": \"High\"}\n",
        ")\n",
        "\n",
        "document_10 = Document(\n",
        "    page_content=\"LangChain continues to evolve with new integrations, including support for databases, APIs, and external data sources, enabling more complex and efficient workflows for AI applications.\",\n",
        "    metadata={\"topic\": \"LangChain\", \"category\": \"Development\", \"features\": [\"Database Integration\", \"API Support\", \"External Data Sources\"]}\n",
        ")\n",
        "\n",
        "documents = [\n",
        "    document_1, document_2, document_3, document_4, document_5,\n",
        "    document_6, document_7, document_8, document_9, document_10\n",
        "]\n"
      ],
      "metadata": {
        "id": "sLrXsh6i-XQe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(  documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMecEO6DEyFH",
        "outputId": "f967babd-d5e5-4889-96a6-744e97f4a641"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Documents to Vector Store and Performing Similarity Search  \n",
        "This code assigns unique IDs to the documents, adds them to the Pinecone vector store, and performs a similarity search for the query **\"What is agent?\"**. The top result's content is displayed.\n"
      ],
      "metadata": {
        "id": "pnDMKjysbUHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "uuid4()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1NDT1oiEoko",
        "outputId": "e144d2b0-7cd9-4917-b019-649de6c74c67"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UUID('c3e34555-4e9b-4f97-9957-9e2d3c8cd7d4')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uuids = [str(uuid4()) for i in range (len(documents))]\n",
        "vector_store.add_documents(documents=documents, ids=uuids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RquzIMr0E1sN",
        "outputId": "671ca133-a7f4-4bd2-8cef-050d48edc239"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['f525a4c7-3d1d-4022-adae-fc35e0271827',\n",
              " '2a608371-5124-4329-a87e-c5f320a7b56c',\n",
              " '23005770-74b6-400d-97a8-bc6ee4ef9cc2',\n",
              " '6b694f87-7a24-4645-8ea6-60cd3c414540',\n",
              " 'b8e74e42-626c-480e-b420-ea4303662aba',\n",
              " '420c6e96-9802-42c6-a765-cf2957f5887b',\n",
              " 'd14416a2-90a6-48a0-8f96-4fd6792ff62e',\n",
              " 'cf8c5a53-331c-43ec-a096-2bada50d4164',\n",
              " '08cdbe4a-4b11-4248-8bc0-885ed7164f2a',\n",
              " '10263394-8628-4d7f-bf80-c4cbbde96088']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_result = vector_store.similarity_search(\n",
        "    \"What is agent?\", k=7,)\n",
        "print(vector_result[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Hh2hyKCFcl6",
        "outputId": "249017df-0c05-4c21-8ffa-81f664ebd573"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agentic AI refers to autonomous AI systems that are capable of decision-making, learning, and adaptation in real-world environments without needing constant human intervention.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Contextual Answers with Google Generative AI  \n",
        "This snippet defines a function to retrieve relevant documents from the vector store and uses Google's **Gemini 2.0** model to generate an answer based on the provided context. The response is displayed in Markdown format.\n"
      ],
      "metadata": {
        "id": "t55IhPV3cJTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "VCi4vqUS5eaf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def user_answer(question):\n",
        "\n",
        "  vector_result = vector_store.similarity_search(question, k=5)\n",
        "\n",
        "  llm = ChatGoogleGenerativeAI(\n",
        "      api_key=GEMINI_KEY,\n",
        "      model = \"gemini-2.0-flash-exp\",\n",
        "      max_tokens= 100,\n",
        "      temperature=0.7\n",
        "  )\n",
        "\n",
        "  prompt1= PromptTemplate(\n",
        "      input_variables=[\"question\"],\n",
        "      template = \"Using this data {vector_result} . Answer the following question: \\n\\n{question}\"\n",
        "  )\n",
        "  chain1= prompt1 | llm\n",
        "\n",
        "  final_answer = chain1.invoke({\"vector_result\": vector_result, \"question\": question})\n",
        "\n",
        "  return final_answer"
      ],
      "metadata": {
        "id": "h6AKlz2P59yM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = user_answer(\"What's the latest news?\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ROL8pmS27d12"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "z09QFgku7imb",
        "outputId": "a623dbb9-507b-46e4-b516-631e09c4b1db"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the provided data, here's a summary of the latest news:\n\n*   **Innovative AI:** A new tech company has developed an innovative AI that solves real-world problems faster than previous models.\n*   **LangChain Evolution:** LangChain is adding new integrations, including support for databases, APIs, and external data sources.\n*   **AI in Customer Service:** AI-powered chatbots are outperforming human customer service agents in resolving technical issues.\n*  **Agent"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_1 = user_answer(\"What is Langchain and where it is used ?\")\n"
      ],
      "metadata": {
        "id": "CTK0435lMLvC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response_1.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "id": "0-3RXtyRMfD9",
        "outputId": "bc76b96b-aacb-44a1-9f5c-7ca609d05ea6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the provided documents, here's what we can say about LangChain and its uses:\n\n**What is LangChain?**\n\n*   LangChain is a framework designed for building applications that utilize Large Language Models (LLMs) like GPT.\n*   It provides tools to manage key components of AI applications, such as:\n    *   **Chains:**  Presumably, this refers to sequences of operations or tasks using LLMs.\n    *   **Agents:**"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response_2 = user_answer(\"What i like in breakfast , Tell me in Key points\")"
      ],
      "metadata": {
        "id": "v8Yuo6iZM-wQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(response_2.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "KTm7YsKJNF6f",
        "outputId": "b0bcc9c3-a1f7-4fa5-dbaf-637be2efbbe1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, based on the provided document data, here are the key points about what you like for breakfast:\n\n*   **Chocolate chip pancakes:** This is a specific item you enjoy.\n*   **Scrambled eggs:** Another breakfast item you like.\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}